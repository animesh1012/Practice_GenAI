{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qdrant Langchain integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup\n",
    "There are various modes of how to run Qdrant, and depending on the chosen one, there will be some subtle differences. The options include:\n",
    "\n",
    "- Local mode, no server required\n",
    "- Docker deployments\n",
    "- Qdrant Cloud\n",
    "\n",
    "```bash\n",
    "pip install -qU langchain-qdrant\n",
    "\n",
    "pip install -qU langchain-huggingface\n",
    "\n",
    "pip install fastembed\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load Embedding Model using GPU/CPU\n",
    "##HuggingFaceEmbeddings: broad model support, GPU acceleration, full Transformers ecosystem.\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"thenlper/gte-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FastEmbedEmbeddings: CPU‐optimized, lightweight, no heavy deps, quantized ONNX inference, ideal for large‐scale offline encoding.\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "\n",
    "embeddings = FastEmbedEmbeddings(model_name=\"thenlper/gte-large\",parallel=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Langchain Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "document_1 = Document(\n",
    "    page_content=\"I had chocolate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "document_2 = Document(\n",
    "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees Fahrenheit.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    ")\n",
    "\n",
    "document_3 = Document(\n",
    "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "document_4 = Document(\n",
    "    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    ")\n",
    "\n",
    "document_5 = Document(\n",
    "    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "document_6 = Document(\n",
    "    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n",
    "    metadata={\"source\": \"website\"},\n",
    ")\n",
    "\n",
    "document_7 = Document(\n",
    "    page_content=\"The top 10 soccer players in the world right now.\",\n",
    "    metadata={\"source\": \"website\"},\n",
    ")\n",
    "\n",
    "document_8 = Document(\n",
    "    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "documents = [\n",
    "    document_1,\n",
    "    document_2,\n",
    "    document_3,\n",
    "    document_4,\n",
    "    document_5,\n",
    "    document_6,\n",
    "    document_7,\n",
    "    document_8,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Qdrant Vectorstore via Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore, RetrievalMode\n",
    "\n",
    "vectorstorestore = QdrantVectorStore.from_documents(\n",
    "    documents,\n",
    "    embeddings,\n",
    "    # url=url,\n",
    "    prefer_grpc=True,\n",
    "    # api_key=api_key,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"my_documents\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_qdrant.qdrant.QdrantVectorStore at 0x712c1005c250>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstorestore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding content to Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_9 = Document(\n",
    "    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    ")\n",
    "\n",
    "document_10 = Document(\n",
    "    page_content=\"I have a bad feeling I am going to get deleted :(\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d206519fcdce4214834914a8b04ef32e', '6ad78e6c41574668b601d62ce3c44921']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_docs = [document_9,document_10]\n",
    "vectorstorestore.add_documents(new_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_qdrant.qdrant.QdrantVectorStore at 0x712c1005c250>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstorestore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity Search in Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Building an exciting new project with LangChain - come check it out! [{'source': 'tweet', '_id': 'da7ebdb8e47e48b7a464b2c391e29f79', '_collection_name': 'my_documents'}]\n",
      "* LangGraph is the best framework for building stateful, agentic applications! [{'source': 'tweet', '_id': 'ab22be7e983b462b99e2d131cfd7d963', '_collection_name': 'my_documents'}]\n"
     ]
    }
   ],
   "source": [
    "results = vectorstorestore.similarity_search(\n",
    "    \"LangChain provides abstractions to make working with LLMs easy\", k=2\n",
    ")\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updating Existing Collection from Qdrant Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_url = \"QDRANT_URL\"\n",
    "qdrant_api_key = \"QDRANT_SERVER_API_KEY\"\n",
    "qdrant_collection_name = \"existing_collection_name\"\n",
    "\n",
    "vector_store = QdrantVectorStore.from_existing_collection(\n",
    "    embedding = embeddings,\n",
    "    collection_name=qdrant_collection_name,\n",
    "    url=qdrant_url,\n",
    "    retrieval_mode=RetrievalMode.HYBRID,\n",
    "    api_key=qdrant_api_key\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_docs = [document_9,document_10]\n",
    "vectorstorestore.add_documents(new_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hybrid Vector Search\n",
    "To perform a hybrid search using dense and sparse vectors with score fusion,\n",
    "\n",
    "- The retrieval_mode parameter should be set to RetrievalMode.HYBRID.\n",
    "- A dense embeddings value should be provided to the embedding parameter.\n",
    "- An implementation of the SparseEmbeddings interface using any sparse embeddings provider has to be provided as a value to the sparse_embedding parameter.\n",
    "- Note that if you've added documents with the HYBRID mode, you can switch to any retrieval mode when searching, since both the dense and sparse vectors are available in the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_list = [\"List of Langchain Documents\"]\n",
    "collection_name = \"qdrant_collection_name\"\n",
    "qdrant_api_key = \"Qdrant_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain_qdrant import FastEmbedSparse, RetrievalMode, QdrantVectorStore\n",
    "\n",
    "sparse_embeddings = FastEmbedSparse(\n",
    "        model_name=\"Qdrant/bm42-all-minilm-l6-v2-attentions\",\n",
    "    )\n",
    "\n",
    "embedding_model = FastEmbedEmbeddings(model_name=\"thenlper/gte-large\",parallel=0)\n",
    "\n",
    "vector_store = QdrantVectorStore.from_documents(\n",
    "        documents = documents_list,\n",
    "        sparse_embedding = sparse_embeddings,\n",
    "        embedding = embedding_model,\n",
    "        collection_name = collection_name,\n",
    "        url = qdrant_url,\n",
    "        retrieval_mode = RetrievalMode.HYBRID,\n",
    "        api_key = qdrant_api_key\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "excel-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
